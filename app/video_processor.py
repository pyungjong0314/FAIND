import cv2
import datetime
import numpy as np
from ultralytics import YOLO
from deep_sort_realtime.deepsort_tracker import DeepSort
from config import YOLO_MODEL_PATH, CONFIDENCE_THRESHOLD, OUTSIDE_LINE, INSIDE_LINE
from utils import apply_mosaic_to_head
from models import Person
import os

# YOLO ëª¨ë¸ ë° ì¶”ì ê¸° ì´ˆê¸°í™”
model = YOLO(YOLO_MODEL_PATH)
tracker = DeepSort(max_age=50)

# ì–¼êµ´ ê²€ì¶œê¸° ë¡œë“œ
face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + "haarcascade_frontalface_default.xml")

# ì´ë™ ë°©í–¥ ê°ì§€
persons = {}
exit_persons = []
entry_persons = []

CONFIDENCE_THRESHOLD = 0.6
GREEN = (0, 255, 0)
WHITE = (255, 255, 255)
RED = (0, 0, 255)


BASE_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_DIR = os.path.abspath(os.path.join(BASE_DIR, ".."))

def process_video(video_path=os.path.join(PROJECT_DIR, "test_video3.mp4")):
    global OUTSIDE_LINE, INSIDE_LINE, persons, exit_persons, entry_persons

    video_cap = cv2.VideoCapture("test_video3.mp4")

    frame_count = 0

    # ì²« ë²ˆì§¸ í”„ë ˆì„ì—ì„œ ì˜ìƒ í¬ê¸° í™•ì¸í•˜ì—¬ ì„  ìœ„ì¹˜ ì„¤ì •
    ret, frame = video_cap.read()
    if not ret:
        return {"status": "Error: ì˜ìƒ íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}

    frame_width = frame.shape[1]
    OUTSIDE_LINE = frame_width // 3       # 1/3 ì§€ì 
    INSIDE_LINE = (frame_width * 2) // 3     # 2/3 ì§€ì 

    while True:
        start = datetime.datetime.now() # FPS ì¸¡ì •ì„ ìœ„í•œ ì‹œê°„
        ret, frame = video_cap.read()
        if not ret:
            break

        frame_count += 1

        results = model(frame)[0]
        detections = [] # ì‚¬ëŒ ê°ì§€ë¥¼ ìœ„í•œ ë¦¬ìŠ¤íŠ¸íŠ¸
        detected_items = []  # ì†Œì§€í’ˆ ê°ì§€ë¥¼ ìœ„í•œ ë¦¬ìŠ¤íŠ¸
        
        for data in results.boxes.data.tolist():
            xmin, ymin, xmax, ymax, confidence, class_id = data
            if confidence < CONFIDENCE_THRESHOLD:
                continue
            class_name = results.names[class_id]
            
            width, height = xmax - xmin, ymax - ymin
            bbox = [xmin, ymin, width, height]
            
            if class_name == "person":
                detections.append((bbox, confidence, class_id))
            else:
                detected_items.append((class_name, bbox))  # ì†Œì§€í’ˆ ì €ì¥

        tracks = tracker.update_tracks(detections, frame=frame)

        for track in tracks:
            if not track.is_confirmed():
                continue

            track_id = track.track_id
            xmin, ymin, xmax, ymax = map(int, track.to_ltrb())
            center_x = (xmin + xmax) // 2

            if track_id not in persons:
                persons[track_id] = Person(track_id, center_x)

            person = persons[track_id]
            person.update_position(center_x)
            
            # **ì‚¬ëŒê³¼ ê²¹ì¹˜ëŠ” ì†Œì§€í’ˆ ì €ì¥**
            for item_name, (ixmin, iymin, ixmax, iymax) in detected_items:
                person.items.add(item_name)  # ì†Œì§€í’ˆ ì¶”ê°€
                # if (xmin < ixmin < xmax and ymin < iymin < ymax) or (xmin < ixmax < xmax and ymin < iymax < ymax):
                #     person.items.add(item_name)  # ì†Œì§€í’ˆ ì¶”ê°€

            # ì´ë™ ë°©í–¥ì´ íŠ¹ì •ëœ ê²½ìš°, ëª¨ìì´í¬ ì²˜ë¦¬ í›„ ì´ë¯¸ì§€ ì €ì¥
            if person.direction and person.direction not in person.passed_lines:
                frame_copy = frame.copy()

                # ì–¼êµ´ ê°ì§€ ë° ëª¨ìì´í¬ ì²˜ë¦¬
                gray_frame = cv2.cvtColor(frame_copy, cv2.COLOR_BGR2GRAY)
                faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))
                
                frame_copy = apply_mosaic_to_head(frame_copy, xmin, ymin, xmax, ymax)
                # ë°”ìš´ë”© ë°•ìŠ¤ ì˜ì—­ë§Œ í¬ë¡­ í›„ ì €ì¥
                person_crop = frame_copy[ymin:ymax, xmin:xmax]  # ë°”ìš´ë”© ë°•ìŠ¤ ì˜ì—­ ìë¥´ê¸°
                
                if person_crop.size != 0: # ë¹ˆ ì´ë¯¸ì§€ ë°©ì§€
                    filename = f"{person.direction}_{track_id}.jpg"
                    cv2.imwrite(f"images/{filename}", person_crop)

                if person.direction == "exit":
                    exit_persons.append({"image": filename, "items": list(person.items)})
                else:
                    entry_persons.append({"image": filename, "items": list(person.items)})

                person.passed_lines.append(person.direction) 

            # Bounding Box ê·¸ë¦¬ê¸°
            cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), GREEN, 2)
            cv2.putText(frame, f"{track_id} - person", (xmin + 5, ymin - 8),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, WHITE, 2)

        # ğŸ”¥ ë‘ ê°œì˜ ê°€ìƒì˜ ì„ ì„ ì˜ìƒì— í‘œì‹œ (ì…êµ¬/ì¶œêµ¬ ë¼ë²¨ ì¶”ê°€)
        cv2.line(frame, (OUTSIDE_LINE, 0), (OUTSIDE_LINE, frame.shape[0]), RED, 2)
        cv2.putText(frame, "outside line", (OUTSIDE_LINE - 40, 30),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, RED, 2)

        cv2.line(frame, (INSIDE_LINE, 0), (INSIDE_LINE, frame.shape[0]), RED, 2)
        cv2.putText(frame, "inside line", (INSIDE_LINE - 20, 30),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, RED, 2)
        
        
        end = datetime.datetime.now()
        fps = f"FPS: {1 / (end - start).total_seconds():.2f}"
        cv2.putText(frame, fps, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 0, 255), 8)
        
        cv2.imshow("YOLO Tracking", frame)
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break


    video_cap.release()
    cv2.destroyAllWindows()

    return {
        "status": "YOLO ì‹¤í–‰ ì™„ë£Œ",
        "entry_person": entry_persons,
        "exit_person": exit_persons
    }
